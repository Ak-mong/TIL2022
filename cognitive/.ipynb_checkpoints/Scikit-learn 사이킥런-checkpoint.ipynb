{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4419decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킥 런을 불러와서 사용하는 방법\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32315f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris #패키지(sklearn.datasets)안에서 데이터(load_iris)를 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e274c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = load_iris() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bdadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset key:dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(f'iris_dataset key:{iris_dataset.keys()}' ) #key들이 뭐가 있는지\n",
    "#print('iris_dataset key:' + ) 해도됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf71e982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# print(iris_dataset['data']) #key값\n",
    "print(iris_dataset['data'].shape) #4개의 속성의 150개의 데이터 (꽃의 길이 등등)\n",
    "\n",
    "# 각각의 데이터를 확인하는것\n",
    "print(iris_dataset['feature_names']) #각각의 항목에 대한 설명을 보여줌\n",
    "print(iris_dataset['target']) #150개의 데이터를 3가지 종류라서 0 1 2 로 보여줌(산토스 등등)\n",
    "print(iris_dataset['target_names']) #타겟의 이름 확인\n",
    "print(iris_dataset['DESCR']) #해당데이터셋에 대한 설명 (..부터 시작하는 설명)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9046d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #학습용 데이터를 쪼개주는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899bea67",
   "metadata": {},
   "source": [
    "학습용 데이터와 테스트용 데이터을 <br>\n",
    "학습용 feature set과 학습용 타겟 <br>\n",
    "테스트용 featrure set과 테스트용 타겟 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d769156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'], #전체 feature set\n",
    "                iris_dataset['target'],\n",
    "                test_size=0.25, #테스트셋의 사이즈 = 학습셋 사이즈가 0.75가 된다\n",
    "                random_state=42 ) #강도(세기)를 결정     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "(112,)\n",
      "(38,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape) #총데이터는 150 개였고 112/38로 쪼갰음, feature은 그대로 4개\n",
    "print(test_input.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e06b9",
   "metadata": {},
   "source": [
    "개 한무더기 고양이 한무더기로 되있을때\n",
    "어떤 새로운 점이 어느 데이터에 가깝냐로 데이터를 분류하는 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e569599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # 최근접 알고리즘\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3) #이제부터 knn이 알고리즘, 몇개를 기준으로 분류할 것인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ec57e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(train_input, train_label) # 학습시작 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ed2ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = knn.predict(test_input) #knn은 학습이 끝난 상태 = 예측을 시작해야됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a40a8b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict_label) #test값을 주었을떄 예측된 값\n",
    "print(test_label)    #실제 값 , 이 둘을 비교해서 정확도를 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11a67886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracyy:1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f'test accuracy:{np.mean(predict_label == test_label)}') \n",
    "# f 라고 쓰면 코드가 있는것? 먼소리일까. 얼마나 일치하는지 평균을 내서 정확도를 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008df29",
   "metadata": {},
   "source": [
    "80의 학습용 데이터와 20의 테스트용 데이터가 있는데 우리는 20의 테스트용 데이터만 가지고해서 쉽다고 카더라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f359826",
   "metadata": {},
   "source": [
    "생 데이터를 그대로 사용하면 오염및 재대로 된 결과가 나오지못함 <br>\n",
    " 0에서 10까지의 데이터와 100에서 1000까지의 데이터를 비교하는 작업들\n",
    " = 0~1의 데이터로 바꿔주는 작업    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c056c",
   "metadata": {},
   "source": [
    "반복되는 텍스트 데이터들은 Encoding 작업을 통해 0,1,2 등 숫자를 부여하는 방법도 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d84a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
